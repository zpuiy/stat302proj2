---
title: "Project 2: stat302proj2 Tutorial"
author: "Peiyao Zhu (Noel)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{stat302proj2 Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(stat302proj2)
library(ggplot2)
library(dplyr)
```

### Introduction
`stat302proj2` is an `R` package for making statistical data prediction and testing hypothesis of data distribution. This package includs functions for doing sample t-test, creating linear models, calculating k-Nearest Neighbors Cross-Validation and Random Forest Cross-Validation.

Install \texttt{stat302proj2} using:
``` {r eval = FALSE}
# install.packages("devtools")
devtools::install_github("zpuiy/stat302proj2")
library(stat302proj2)
```

### A tutorial for `my_t.test`
* Demonstrate a test of the hypothesis
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &\neq 60.
  \end{align}
```{r}
twoSided_life <- my_t_test(my_gapminder$lifeExp, "two.sided", 60)
twoSided_life
```
  
According to the result, the p-value is `r twoSided_life$p_val`, which is greater than 0.05. I fail to reject the null hypothesis that the average life expectancy is equal to 60. Since I have insufficient evidence to reject the null hypothesis, I don't accept any hypothesis. 
  
  * Demonstrate a test of the hypothesis
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &< 60.
  \end{align}
```{r}
less_life <- my_t_test(my_gapminder$lifeExp, "less", 60)
less_life
```

According to the result, the p-value is `r less_life$p_val`, which is less than 0.05. I reject the null hypothesis that the average life expectancy is equal to 60 and conclude that the true mean of life expectancy is not equal 60.

  *  Demonstrate a test of the hypothesis
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &> 60.
  \end{align}
```{r}
greater_life <- my_t_test(my_gapminder$lifeExp, "greater", 60)
greater_life
```

According to the result, the p-value is `r greater_life$p_val`, which is greater than 0.05. I failed to reject the null hypothesis that the average life expectancy is equal to 60. Since I have insufficient evidence to reject the null hypothesis, I don't accept any hypothesis. 

### A tutorial for `my_lm`

* Demonstrate a regression using `lifeExp` as your response variable and `gdpPercap` and `continent` as explanatory variables.
```{r} 
life_reg <- my_lm(lifeExp ~ gdpPercap + continent, my_gapminder)
life_reg
```

The gdpPercap coefficient in the linear regression is `r life_reg[2, 1]`. The gdpPercap coefficient represents the estimate mean increase in life expectancy in year for every additional one unit increase in gdp per captita. If gdp per capita increases by one unit, assuming that the continent remains constant, the expect value of life expectancy will increase by `r life_reg[2, 1]` year.

* The hypothesis test associated with the `gdpPercap` coefficient.

According to the result, the p-value is `r life_reg[2, 4]`, which is less than 0.05. Therefore, I reject the null hypothesis that the coefficient for gdp per capita is equal to 0 and conclude that the coefficient for gdp per capita is not equal 60.

* Use `ggplot2` to plot the Actual vs. Fitted values.
```{r}
mod_fits <- fitted(lm(lifeExp ~ gdpPercap + continent, my_gapminder))
my_df <- data.frame(actual = my_gapminder$lifeExp, fitted = mod_fits)
ggplot(my_df, aes(x = fitted, y = actual)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, col = "red", lty = 2) + 
  theme_bw(base_size = 15) +
  labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  theme(plot.title = element_text(hjust = 0.5))
```
The slope of the fitted value is generally goes well with the trend of the actual value, however, the model seems to overpredict low values and under predict high values of the life expectancy. The actual and predicted life expectancy doesn't seem very correlated.

### A tutorial for `my_knn_cv` using `my_penguins`

* Predict output class `species` using covariates `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. 
* Use $5$-fold cross validation (`k_cv = 5`).
* Iterate from `k_nn`$= 1,\ldots, 10$:
    * For each value of `k_nn`, record the training misclassification rate and the CV misclassification rate (output from your function).
```{r}
err_table <- matrix(data = NA, nrow = 10, ncol = 2)
rownames(err_table) <- c("k_nn = 1", "k_nn = 2", "k_nn = 3", "k_nn = 4", 
                         "k_nn = 5", "k_nn = 6", "k_nn = 7", "k_nn = 8",
                         "k_nn = 9", "k_nn = 10")
colnames(err_table) <- c("CV misclassification error", "Training error")
mod_penguins <- my_penguins %>% na.omit()
input_penguins <- mod_penguins %>% 
  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)
for (i in 1:10) {
  err_table[i, 1] <- my_knn_cv(input_penguins, mod_penguins$species, i, 5)$cv_err
  err_table[i, 2] <- my_knn_cv(input_penguins, mod_penguins$species, i, 5)$train_err
}
err_table
```

Based on the CV misclassification error rates, I would choose the 1-nearest neighbor model. Based on the training error rates,I would choose the 1-nearest neighbor model.
I would choose the 1-nearest neighbor model because it has lower CV misclassification error rate and lower training error rate.
The process of cross-validation splits data into k folds, then use all but 1 fold as the training data and fit the model. The remaining fold for the test data and make predictions. It switches which fold is the test data until all folds have been test data (k times). After making predictions, it calculates the CV error. It is useful because it allows us to choose among several models based on this test error in order to find the best model for making predictions.

### A tutorial for `my_rf_cv`
* Predict `body_mass_g` using covariates `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm`. 
* Iterate through `k` in  `c(2, 5, 10)`:
    * For each value of `k`, run your function $30$ times. 
    * For each of the $30$ iterations, store the CV estimated MSE.
```{r}
cv_estimate <- list()
trial <- 1
for (i in c(2, 5, 10)) {
  mse <- rep(NA, 30)
  for (j in 1:30) {
    mse[j] <- my_rf_cv(i)
  }
  cv_estimate[[trial]] <- mse
  trial <- trial + 1
}
```

* Use `ggplot2` with 3 boxplots to display these data in an informative way. There should be a boxplot associated with each value of `k`, representing $30$ simulations each.
```{r}
category <- c(rep("2", 30), rep("5", 30), rep("10", 30))
mse <- c(cv_estimate[[1]], cv_estimate[[2]], cv_estimate[[3]])
my_cv <- data.frame("k" = category, "CV_MSE" = mse)
ggplot(data = my_cv, aes(x = k, y = CV_MSE)) +
  geom_boxplot(aes(group = k), fill = "lightblue") +
  labs(title = "CV MSE by k value", 
       x = "", 
       y = "") +
  theme(plot.title =
          element_text(hjust = 0.5))
```

* Use a table to display the average CV estimate and the standard deviation of the CV estimates across $k$. Your table should have 3 rows (one for each value of $k$) and 2 columns (one for the mean and the other for the standard deviation).
```{r}
average_cv <- matrix(NA, nrow = 3, ncol = 2)
rownames(average_cv) <- c("k = 2", "k = 5", "k = 10")
colnames(average_cv) <- c("average CV MSE", "standard deviation")
average_cv[1, 1] <- mean(cv_estimate[[1]])
average_cv[1, 2] <- sd(cv_estimate[[1]])
average_cv[2, 1] <- mean(cv_estimate[[2]])
average_cv[2, 2] <- sd(cv_estimate[[2]])
average_cv[3, 1] <- mean(cv_estimate[[3]])
average_cv[3, 2] <- sd(cv_estimate[[3]])
average_cv <- as.data.frame(average_cv)
average_cv
```

I observe tht as the k value gets larger, the range, average value and standard deviation of the CV estimated MSE gets smaller. From the boxplot, I also observe that as k gets larger, the overall values of CV estimated MSE tends to get smaller. As we split data in to more folds, we will get predictions that are closer to the actual results, therefore, the variance of our prediction will get smaller. 
